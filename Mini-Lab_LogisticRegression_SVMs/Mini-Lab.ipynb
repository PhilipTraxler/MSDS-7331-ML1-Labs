{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Title:__ Mini-Lab: Logistic Regression and SVMs  \n",
    "__Authors:__ Butler, Derner, Holmes, Traxler  \n",
    "__Date:__ 1/22/23 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "You are to perform predictive analysis (classification) upon a data set: model the dataset using \n",
    "methods we have discussed in class: logistic regression & support vector machines and making \n",
    "conclusions from the analysis. Follow the CRISP-DM framework in your analysis (you are not \n",
    "performing all of the CRISP-DM outline, only the portions relevant to the grading rubric outlined \n",
    "below). This report is worth 10% of the final grade. You may complete this assignment in teams \n",
    "of as many as three people. \n",
    "Write a report covering all the steps of the project. The format of the document can be PDF, \n",
    "*.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in \n",
    "the rendered Jupyter notebook. The results should be reproducible using your report. Please \n",
    "carefully describe every assumption and every step in your report.\n",
    "A note on grading: A common mistake I see in this lab is not investigating different input \n",
    "parameters for each model. Try a number of parameter combinations and discuss how the \n",
    "model changed. \n",
    "SVM and Logistic Regression Modeling \n",
    " - [50 points] Create a logistic regression model and a support vector machine model for the \n",
    "classification task involved with your dataset. Assess how well each model performs (use \n",
    "80/20 training/testing split for your data). Adjust parameters of the models to make them \n",
    "more accurate. If your dataset size requires the use of stochastic gradient descent, then \n",
    "linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing \n",
    "logistic regression and linear support vector machines. For many problems, SGD will be \n",
    "required in order to train the SVM model in a reasonable timeframe. \n",
    " - [10 points] Discuss the advantages of each model for each classification task. Does one \n",
    "type of model offer superior performance over another in terms of prediction accuracy? In \n",
    "terms of training time or efficiency? Explain in detail. \n",
    " - [30 points] Use the weights from logistic regression to interpret the importance of different \n",
    "features for the classification task. Explain your interpretation in detail. Why do you think \n",
    "some variables are more important? \n",
    " - [10 points] Look at the chosen support vectors for the classification task. Do these provide \n",
    "any insight into the data? Explain. If you used stochastic gradient descent (and therefore did \n",
    "not explicitly solve for support vectors), try subsampling your data to train the SVC model—\n",
    "then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CRISP-DM__\n",
    " - Business understanding – What does the business need?\n",
    " - Data understanding – What data do we have / need? Is it clean?\n",
    " - Data preparation – How do we organize the data for modeling?\n",
    " - Modeling – What modeling techniques should we apply?\n",
    " - Evaluation – Which model best meets the business objectives?\n",
    " - Deployment – How do stakeholders access the results?\n",
    "\n",
    " Source: [Hotz, 2023](https://www.datascience-pm.com/crisp-dm-2/)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Buisness Understanding__  \n",
    "What features are most important in predicting which flights will be delayed?\n",
    "\n",
    "__Data Understanding__  \n",
    "We have a dataset of over 200,000 flights and 60 features from US Carriers in 2021. Approximately 33% of flights are delayed. There are some features with null values. These features are largely associated with columns that will be removed for the simple reason that they would not be knowable prior to the flight. We will have to prune the dataset to only include knowable features. Some of the features are highly correlated because they represent very similar things (e.g. scheduled departure time(CRSDepTime) vs. actual departure time(DepTime)). While most of these will be removed because they are not knowable, the remaining will be chosen based on the completeness of the data. The remaining issue to address is multi-colinearity. The majority of correlated features will be removed in the above steps. The remaining will again be chosen by the completeness of their data. Any remaining observations with incomplete data will be removed.\n",
    "\n",
    "__Data Preparation__  \n",
    "All categorical features will require one hot encoding. Some will not be usable due to the number of levels compared to the number of observations available.\n",
    "\n",
    "The continous features will be normalized to reduce the influence of features with large values.\n",
    "\n",
    "__Modeling__  \n",
    "We will be comparing Logistic Regression and Support Vector Machines in this notebook. Each model will use the same training and testing datasets.\n",
    "\n",
    "__Evaluation__  \n",
    "The overall performance of each model will be evaluated by their respective accuracy, sensitivity, and specificity. We have concluded that it is more important to accurately predict the true occurance of delayed flights. For this reason, we will use sensitivity as the primary metric to compare and evaluate each model.\n",
    "\n",
    "__Deployment__  \n",
    "The findings of our study, including important features and their weights, can be found in the conclusion section of this rendered Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import metrics\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "url = 'https://github.com/cdholmes11/MSDS-7331-ML1-Labs/blob/main/Lab-1_Visualization_DataPreprocessing/data/Combined_Flights_2021_sample.csv?raw=true'\n",
    "flight_data_df = pd.read_csv(url, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Features\n",
    "flight_data_df['Delayed'] = np.where(flight_data_df['DepDelayMinutes'] > 0, 1, 0)\n",
    "flight_data_df['Dif_Oper'] = np.where(flight_data_df['DOT_ID_Marketing_Airline'] == flight_data_df['DOT_ID_Operating_Airline'], 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset doesn't have a feature for binary classification of delayed status. We've added one based on the DepDelayMinutes field. Delayed flights are coded as 1 and not delayed as 0.\n",
    "\n",
    "Marketing Airline and Operatin Airline are often the same. One will likely be dropped from the model due to multi-colinearity. To maintain the important information from both columns, we've added a classification feature for when the flight is marketed by one airline and operated by another. Flights with different operators are coded as 1 and 0 for flights with the same marketing airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Shape\n",
    "flight_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delayed Frequency\n",
    "delayed_df = pd.DataFrame(flight_data_df['Delayed'].value_counts()).reset_index()\n",
    "delayed_df.columns = ['Delayed', 'Count']\n",
    "delayed_df['Frequency'] = round(delayed_df['Count'] / sum(delayed_df['Count']) * 100, 2)\n",
    "delayed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowable features grouped by data type\n",
    "cat_features = ['Airline', 'Dest', 'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'DestCityName', 'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'DOT_ID_Marketing_Airline', 'DOT_ID_Operating_Airline', 'Flight_Number_Marketing_Airline', 'Flight_Number_Operating_Airline', 'IATA_Code_Marketing_Airline', 'IATA_Code_Operating_Airline', 'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners', 'Operating_Airline', 'Origin', 'OriginAirportID', 'OriginAirportSeqID', 'OriginCityMarketID', 'OriginCityName', 'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac', 'Tail_Number']\n",
    "cont_features = ['CRSArrTime', 'CRSDepTime', 'CRSElapsedTime', 'Distance']\n",
    "ord_features = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Delayed', 'Dif_Oper']\n",
    "date_feature = ['FlightDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimmed Dataset to knowable features\n",
    "flight_delay_df = flight_data_df[cat_features + cont_features + ord_features + date_feature]\n",
    "flight_delay_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Plot using Plotly\n",
    "flight_corr = flight_delay_df.corr()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x = flight_corr.columns,\n",
    "        y = flight_corr.index,\n",
    "        z = np.array(flight_corr),\n",
    "        text=flight_corr.values,\n",
    "        texttemplate='%{text:.2f}' #set the size of the text inside the graphs\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Airline Feature Correlation',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features converted to corresponding group type\n",
    "flight_data_df[cat_features] = flight_data_df[cat_features].astype('category')\n",
    "flight_data_df[ord_features] = flight_data_df[ord_features].astype(np.int64)\n",
    "flight_data_df[cont_features] = flight_data_df[cont_features].astype(np.int64)\n",
    "flight_data_df['FlightDate'] = pd.to_datetime(flight_data_df['FlightDate']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail Number Evaluation\n",
    "flight_delay_df['Tail_Number'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail Number Delay Frequency\n",
    "tail_df = pd.pivot_table(flight_delay_df, values='Distance', index =['Tail_Number'], aggfunc = 'count', columns='Delayed').reset_index()\n",
    "tail_df.columns = ['Tail_Number', 'On-Time', 'Delayed']\n",
    "tail_df['Total'] = tail_df['Delayed'] + tail_df['On-Time']\n",
    "tail_df['Frequency'] = round((tail_df['Delayed'] / tail_df['Total'] * 100),2)\n",
    "\n",
    "tail_df[tail_df['Total'].notnull()].sort_values(by=['Delayed'], ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many categorical features have a corresponding numeric feature. Wherever possible, we've decided to utilize the numeric representation of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat_remove = ['Airline', 'Dest', 'DestCityName', 'DestState', 'DestStateName', \n",
    "    'DOT_ID_Operating_Airline', 'Flight_Number_Operating_Airline', 'IATA_Code_Marketing_Airline',\n",
    "    'IATA_Code_Operating_Airline', 'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n",
    "    'Operating_Airline', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateName', 'DestAirportSeqID',\n",
    "    'OriginAirportSeqID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to remove\n",
    "cat_remove = ['Airline', 'Dest', 'DestCityName', 'DestState', 'DestStateName', \n",
    "    'DOT_ID_Operating_Airline', 'Flight_Number_Operating_Airline', 'IATA_Code_Marketing_Airline',\n",
    "    'IATA_Code_Operating_Airline', 'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n",
    "    'Operating_Airline', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateName', 'DestAirportSeqID',\n",
    "    'OriginAirportSeqID', 'Tail_Number', 'DestCityMarketID']\n",
    "cont_remove = ['CRSArrTime', 'CRSElapsedTime']\n",
    "ord_remove = ['Quarter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features from category groups\n",
    "for n in cat_remove:\n",
    "    if n in cat_features:\n",
    "        cat_features.remove(n)\n",
    "\n",
    "for n in cont_remove:\n",
    "    if n in cont_features:\n",
    "        cont_features.remove(n)\n",
    "\n",
    "for n in ord_remove:\n",
    "    if n in ord_features:\n",
    "        ord_features.remove(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataset with updated feature lists\n",
    "flight_delay_df = flight_data_df[cat_features + cont_features + ord_features]\n",
    "flight_delay_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tail Number has over 5700 unique values. One Hot Encoding Tail Number will add too many features to the dataset, for the given observation count. Additionaly, future datasets will likely contain different tail numbers. Practically, this makes tail number an unrealistic feature to include in the model. For the curious, we've provided a table of tail numbers sourted by the number of times delayed and provided their relative freqency of being delayed. There are clearly some aircraft that are much more likely to be delayed than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay_df[cat_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X = flight_delay_df.drop(['Delayed'], axis=1)\n",
    "Y = flight_delay_df['Delayed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse_output=True)\n",
    "label_encoder = LabelEncoder()\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "X = onehot_encoder.fit_transform(X)\n",
    "Y = label_encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=110)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Logistic Regression\n",
    "logmod = LogisticRegression(random_state=10, max_iter=5000)\n",
    "log_fit = logmod.fit(X_train, Y_train)\n",
    "y_pred = logmod.predict(X_test)\n",
    "\n",
    "# Model Performance\n",
    "print('Basic Score: ', logmod.score(X_train, Y_train))\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.w3schools.com/python/python_ml_confusion_matrix.asp\n",
    "confusion_matrix = confusion_matrix(Y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a precision of 56%, the untuned logistic regression model is an improvement over the delayed frequency of the sample. That is to say, it's true positive rate is greater than 33%. While the overall model accuracy of 70% is barely an improvement from the baseline delayed sample frequency, precision is far more important than accuracy for this model.\n",
    "\n",
    "In our reasearch, we have found the initial feature size, after One Hot Encoding, to be too resource entensive to be practical. Moving forward, we will use the basic logistic model to find the minimum features resulting in similar results. This will greatly reduce the processing requirements for future models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "prec_list = []\n",
    "i = 50\n",
    "while i < 5000:\n",
    "    logmod_feture = LogisticRegression(random_state=10, max_iter=5000)\n",
    "    selector = SelectFromModel(logmod_feture, max_features=i).fit(X_train, Y_train)\n",
    "    \n",
    "    X_train_new = selector.transform(X_train)\n",
    "    X_test_new = selector.transform(X_test)\n",
    "\n",
    "    logmod_feture.fit(X_train_new, Y_train)\n",
    "    y_pred_feature = logmod_feture.predict(X_test_new)\n",
    "\n",
    "    prec_score = precision_score(Y_test, y_pred_feature, average='macro')\n",
    "    prec_list.append([prec_score, i])\n",
    "\n",
    "    i+=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Max Features vs. Precision\n",
    "prec = pd.DataFrame(prec_list)\n",
    "prec.columns = ['Precision', 'Max_Features']\n",
    "\n",
    "max_prec = max(prec['Precision'])\n",
    "best_max_feature = prec['Max_Features'][prec['Precision'] == max_prec]\n",
    "best_max_feature = round(best_max_feature.item(), 2)\n",
    "print(f'The Max Features that produces the best precision is {best_max_feature}')\n",
    "\n",
    "fig = px.line(prec, x=\"Max_Features\", y=\"Precision\", title='Precision by Max_Features' , width=1000)\n",
    "fig.add_vline(x=best_max_feature, line_color=\"red\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmod_trim = LogisticRegression(random_state=10, max_iter=5000)\n",
    "selector = SelectFromModel(logmod_trim, max_features=1000).fit(X_train, Y_train)\n",
    "\n",
    "X_train_new = selector.transform(X_train)\n",
    "X_test_new = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Param\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'none'],\n",
    "    'C' : [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'solver' : ['newton-cholesky','sag','saga'],\n",
    "    'max_iter' : [2000, 5000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Logistic Regression\n",
    "logmod_cv = LogisticRegression(random_state=10)\n",
    "clf = GridSearchCV(\n",
    "    logmod_cv,\n",
    "    param_grid = param_grid, \n",
    "    cv=None ,\n",
    "    verbose=False,\n",
    "    n_jobs=-1, \n",
    "    scoring='precision',\n",
    "    refit=True\n",
    "    )\n",
    "best_clf = clf.fit(X_train_new, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_final = LogisticRegression(\n",
    "    C=0.1,\n",
    "    max_iter=2000,\n",
    "    penalty= 'l2',\n",
    "    solver='newton-cholesky'\n",
    ")\n",
    "log_final.fit(X_train_new, Y_train)\n",
    "\n",
    "scores = cross_validate(log_final, X_train_new, Y_train, scoring='precision', cv=10, return_train_score=True)\n",
    "y_pred_final = cross_val_predict(log_final, X_test_new, Y_test, cv=10)\n",
    "\n",
    "print(cross_val_score(log_final, X_test_new, Y_test, cv=10))\n",
    "print(classification_report(Y_test, y_pred_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters, we were able to improve our precision by 6%. Since l2 is the default penalty, the two primary changes were to C and the solver. A reduced C from 1 to 0.1 and changing the solver from lbfgs to newton-cholesky provided the above results. This is in alignment with the scikit-learn documentation that states 'newton-cholesky' is a \"good choice for one-hot encoded categoircal features with rare categories\" (Sklearn.Linear_Model.LogisticRegression, n.d.)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = svm.SVC()\n",
    "\n",
    "svc_model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = log_final.coef_[0]\n",
    "data = {'Coef': importance,'Feature': X_train_new[0]}\n",
    "df2 = pd.DataFrame(data)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "1. Hotz, N. (2023, January 19). What is CRISP DM? Data Science Process Alliance. https://www.datascience-pm.com/crisp-dm-2/\n",
    "2. sklearn.linear_model.LogisticRegression. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77fad8f1fd48b0dbc17e5e0b2f14396946f41876e8f98b3588ed05859c665f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
